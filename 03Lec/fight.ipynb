{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7154ac1e-7ac9-4def-90d1-f73e2268621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python --version\n",
    "#! pip install gym==0.21.0\n",
    "#! pip install gym-retro==0.8.0\n",
    "# !pip install stable-baselines3[extra]\n",
    "# !pip install gymnasium[all]\n",
    "# !pip install opencv-python\n",
    "#! python -m retro.import .\n",
    "# retro.data.list_games()\n",
    "import retro\n",
    "import numpy as np # to calculate the delta between the frames\n",
    "import cv2 # for grayscaling\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e495aa7c-1170-495b-ab5e-f067ea99aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\")\n",
    "env.observation_space\n",
    "env.action_space\n",
    "env.action_space.sample()\n",
    "obs = env.reset() # Reset game to starting state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8a6b07-3f8e-4aed-b026-0a2efb2d3185",
   "metadata": {},
   "source": [
    "## No training, just random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76337f3b-5594-4c97-b8b0-19638888961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False # Set flag to flase, tells us when we dies\n",
    "for game in range(1): # play one game \n",
    "   while not done: \n",
    "       if done: \n",
    "           obs = env.reset() # when we do die, we start the game again\n",
    "       env.render()\n",
    "       obs, reward, done, info = env.step(env.action_space.sample()) # randomly take action\n",
    "       #time.sleep(0.000000001)\n",
    "       #if reward > 0:\n",
    "           #print(reward) # only a number when win, no rewards when getting along which makes it hard to train rl agent - sparse rewards\n",
    "        #need to change this, as the spare rewards will make it hard to train the rl agent\n",
    "\n",
    "\n",
    "#max score: 6300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4714b22d-7a76-4241-a44f-2234ad28bb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enemy_matches_won': 2,\n",
       " 'score': 5600,\n",
       " 'matches_won': 0,\n",
       " 'continuetimer': 10,\n",
       " 'enemy_health': 0,\n",
       " 'health': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275f322-6855-4141-8b4b-0e3746365d3d",
   "metadata": {},
   "source": [
    "## Random actions, but initialised using a constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163dffce-242d-45f5-8175-6c4ddcbb5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box\n",
    "#Creating custom environment that will carry out all the steps\n",
    "#we pass our pass environment\n",
    "\n",
    "class StreetFighter(Env): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8) \n",
    "        self.action_space = MultiBinary(12) \n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "    \n",
    "    def reset(self):\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs # want to also keep track of the previous frame to calculate a delta between the frames\n",
    "        self.score = 0 \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation): \n",
    "        # Grayscaling \n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize \n",
    "        resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels \n",
    "    \n",
    "    def step(self, action): \n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs) \n",
    "        \n",
    "        # Frame delta - use this to train our agent\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs \n",
    "        # Reshape the reward function\n",
    "        reward = info['score'] - self.score \n",
    "        self.score = info['score'] \n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "env = StreetFighter()\n",
    "env.observation_space\n",
    "env.action_space.sample()\n",
    "\n",
    "obs = env.reset() # Reset game to starting state\n",
    "\n",
    "done = False # Set flag to flase, tells us when we die\n",
    "for game in range(1): # play one game \n",
    "   while not done: \n",
    "       env.render()\n",
    "       obs, reward, done, info = env.step(env.action_space.sample()) # randomly take action 'max score 74900'\n",
    "       #time.sleep(0.000001)\n",
    "       #if reward > 0:\n",
    "           #print(reward) # only a number when win, no rewards when getting along which makes it hard to train rl agent - sparse rewards\n",
    "        #need to change this, as the spare rewards will make it hard to train the rl agent\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a0d818-da83-4401-8811-ef5db89efbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enemy_matches_won': 2,\n",
       " 'score': 37300,\n",
       " 'matches_won': 0,\n",
       " 'continuetimer': 10,\n",
       " 'enemy_health': 0,\n",
       " 'health': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info\n",
    "#max score': 37300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f97c3f-4337-43dc-ab5d-a9365ee8a9b3",
   "metadata": {},
   "source": [
    "## Removing the irrelevant colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec22093-9b5e-4cf5-b11a-2e61b89e6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box\n",
    "#Creating custom environment that will carry out all the steps\n",
    "#we pass our pass environment\n",
    "\n",
    "class StreetFighter(Env): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8) \n",
    "        self.action_space = MultiBinary(12) \n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "    \n",
    "    def reset(self):\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs # want to also keep track of the previous frame to calculate a delta between the frames\n",
    "        self.score = 0 \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        # Convert the image to HSV color space\n",
    "        hsv = cv2.cvtColor(observation, cv2.COLOR_BGR2HSV) \n",
    "        # Define color ranges for red, blue, and green in HSV\n",
    "        lower_red1 = np.array([0, 50, 50])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([170, 50, 50])\n",
    "        upper_red2 = np.array([180, 255, 255])\n",
    "        lower_blue = np.array([100, 50, 50])\n",
    "        upper_blue = np.array([140, 255, 255])\n",
    "        lower_green = np.array([40, 50, 50])\n",
    "        upper_green = np.array([80, 255, 255])       \n",
    "        # Create masks for red, blue, and green colors\n",
    "        mask_red1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "        mask_red2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "        mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "        mask_green = cv2.inRange(hsv, lower_green, upper_green)\n",
    "        # Combine the masks\n",
    "        mask_red = cv2.bitwise_or(mask_red1, mask_red2)\n",
    "        mask = cv2.bitwise_or(mask_red, mask_blue)\n",
    "        mask = cv2.bitwise_or(mask, mask_green)\n",
    "        # Apply the mask to the original image\n",
    "        filtered = cv2.bitwise_and(observation, observation, mask=mask) \n",
    "        # Convert the filtered image to grayscale\n",
    "        gray = cv2.cvtColor(filtered, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize the image\n",
    "        resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "        # Add the channels value\n",
    "        channels = np.reshape(resize, (84, 84, 1))\n",
    "        return channels\n",
    "    \n",
    "    def step(self, action): \n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs) \n",
    "        \n",
    "        # Frame delta - use this to train our agent\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs \n",
    "        # Reshape the reward function\n",
    "        reward = info['score'] - self.score \n",
    "        self.score = info['score'] \n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "env = StreetFighter()\n",
    "env.observation_space\n",
    "env.action_space.sample()\n",
    "\n",
    "obs = env.reset() # Reset game to starting state\n",
    "\n",
    "done = False # Set flag to flase, tells us when we die\n",
    "for game in range(1): # play one game \n",
    "   while not done: \n",
    "       env.render()\n",
    "       obs, reward, done, info = env.step(env.action_space.sample()) # randomly take action 'max score 74900'\n",
    "       #time.sleep(0.000001)\n",
    "       #if reward > 0:\n",
    "           #print(reward) # only a number when win, no rewards when getting along which makes it hard to train rl agent - sparse rewards\n",
    "        #need to change this, as the spare rewards will make it hard to train the rl agent\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074ce9c1-1496-4e86-97f9-4f10e74000e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enemy_matches_won': 2,\n",
       " 'score': 5900,\n",
       " 'matches_won': 0,\n",
       " 'continuetimer': 10,\n",
       " 'enemy_health': 0,\n",
       " 'health': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info\n",
    "#5900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7f991c-ec3b-44e0-b6d1-eac459d099ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasil\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./street_fighter_ppo/PPO_21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m\n\u001b[0;32m     93\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env])\n\u001b[0;32m     94\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./street_fighter_ppo/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_street_fighter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_street_fighter\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m=\u001b[39menv)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:224\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    221\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    222\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[1;32m--> 224\u001b[0m \u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:474\u001b[0m, in \u001b[0;36mRolloutBuffer.add\u001b[1;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(reward)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_starts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(episode_start)\n\u001b[1;32m--> 474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#! python --version\n",
    "#! pip install gym==0.21.0\n",
    "#! pip install gym-retro==0.8.0\n",
    "# !pip install stable-baselines3[extra]\n",
    "# !pip install gymnasium[all]\n",
    "# !pip install opencv-python\n",
    "import retro\n",
    "import numpy as np # to calculate the delta between the frames\n",
    "import cv2 # for grayscaling\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box, MultiBinary\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Define the custom environment class\n",
    "class StreetFighter(Env):\n",
    "    def __init__(self, render_mode=\"human\"):   #, render_mode='human'\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(12)\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "        self.render_mode = render_mode\n",
    "       \n",
    "    def reset(self):\n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "        self.score = 0\n",
    "        return obs\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        # Convert the image to HSV color space\n",
    "        hsv = cv2.cvtColor(observation, cv2.COLOR_BGR2HSV)  \n",
    "        # Define color ranges for red, blue, and green in HSV\n",
    "        lower_red1 = np.array([0, 50, 50])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([170, 50, 50])\n",
    "        upper_red2 = np.array([180, 255, 255])\n",
    "        lower_blue = np.array([100, 50, 50])\n",
    "        upper_blue = np.array([140, 255, 255])\n",
    "        lower_green = np.array([40, 50, 50])\n",
    "        upper_green = np.array([80, 255, 255])\n",
    "        # Create masks for red, blue, and green colors\n",
    "        mask_red1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "        mask_red2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "        mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "        mask_green = cv2.inRange(hsv, lower_green, upper_green) \n",
    "        # Combine the masks\n",
    "        mask_red = cv2.bitwise_or(mask_red1, mask_red2)\n",
    "        mask = cv2.bitwise_or(mask_red, mask_blue)\n",
    "        mask = cv2.bitwise_or(mask, mask_green)    \n",
    "        # Apply the mask to the original image\n",
    "        filtered = cv2.bitwise_and(observation, observation, mask=mask) \n",
    "        # Convert the filtered image to grayscale\n",
    "        gray = cv2.cvtColor(filtered, cv2.COLOR_BGR2GRAY)      \n",
    "        # Resize the image\n",
    "        resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)      \n",
    "        # Add the channels value\n",
    "        channels = np.reshape(resize, (84, 84, 1))   \n",
    "        return channels\n",
    "\n",
    "    \n",
    "    # def preprocess(self, observation):\n",
    "    #     gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "    #     resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "    #     channels = np.reshape(resize, (84, 84, 1))\n",
    "    #     return channels\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs)\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        return frame_delta, reward, done, info\n",
    "     \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "     \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "# Create the environment\n",
    "env = StreetFighter() #StreetFighter(render_mode='human') \n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=\"./street_fighter_ppo/\")\n",
    "model.learn(total_timesteps=1)\n",
    "model.save(\"ppo_street_fighter\")\n",
    "model = PPO.load(\"ppo_street_fighter\", env=env)\n",
    "\n",
    "# Run the game loop with the trained model\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    #time.sleep(0.000001)\n",
    "    if reward > 0:\n",
    "        print(reward)\n",
    "    time.sleep(0.01)  # Slight delay to make the rendering visible    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea60f7-1dd6-4b4a-9840-bbe98ada8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60565977-fc4c-4c76-8264-8164d9a3a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./street_fighter_ppo/PPO_19\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 191  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasil\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.]\n",
      "[500.]\n",
      "[500.]\n",
      "[300.]\n",
      "[500.]\n",
      "[100.]\n",
      "[1000.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[10000.]\n",
      "[500.]\n",
      "[100.]\n",
      "[1000.]\n",
      "[300.]\n",
      "[1000.]\n",
      "[500.]\n",
      "[500.]\n",
      "[500.]\n",
      "[300.]\n",
      "[1000.]\n",
      "[500.]\n",
      "[500.]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, MultiBinary\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Define the custom environment class\n",
    "class StreetFighter(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(12)\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.game.seed(seed)\n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "        self.score = 0\n",
    "        return obs, {}\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84, 84, 1))\n",
    "        return channels\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs)\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        return frame_delta, reward, done, False, info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "# Create the environment\n",
    "env = StreetFighter()\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Define the PPO model with the custom environment\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=\"./street_fighter_ppo/\")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_street_fighter\")\n",
    "\n",
    "# Load the trained model (if not already loaded)\n",
    "model = PPO.load(\"ppo_street_fighter\", env=env)\n",
    "\n",
    "# Run the game loop with the trained model\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    #time.sleep(0.000001)\n",
    "    if reward > 0:\n",
    "        print(reward)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4f27c9-81b2-42c2-82b1-f1dcd9139c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'enemy_matches_won': 2,\n",
       "  'score': 20600,\n",
       "  'matches_won': 1,\n",
       "  'continuetimer': 10,\n",
       "  'enemy_health': 0,\n",
       "  'health': 0,\n",
       "  'TimeLimit.truncated': False,\n",
       "  'terminal_observation': array([[[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "  \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "  \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "  \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       "  \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          ...,\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]]], dtype=uint8)}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info\n",
    "#MAX score 27200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a639040-edb2-4b48-a63a-40fef55dcb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasil\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasil\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`. Alternatively, please install imageio-ffmpeg with `pip install imageio-ffmpeg`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     70\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000000\u001b[39m  \u001b[38;5;66;03m# Adjust the number of timesteps based on your needs\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m     74\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO_StreetFighter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:287\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[0;32m    278\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[0;32m    285\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 287\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:423\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py:113\u001b[0m, in \u001b[0;36mVecTransposeImage.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict]:\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Reset all environments\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observations, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m))\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_observations(observations)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m     76\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m---> 77\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmaybe_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\shimmy\\openai_gym_compatibility.py:235\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     warn(\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGym v21 environment do not accept options as a reset parameter, options=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m     )\n\u001b[1;32m--> 235\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\gym\\wrappers\\monitor.py:56\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_reset()\n\u001b[0;32m     55\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\gym\\wrappers\\monitor.py:241\u001b[0m, in \u001b[0;36mMonitor._after_reset\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Reset the stat count\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_recorder\u001b[38;5;241m.\u001b[39mafter_reset(observation)\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_video_recorder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Bump *after* all reset activity has finished\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\gym\\wrappers\\monitor.py:267\u001b[0m, in \u001b[0;36mMonitor.reset_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Start recording the next video.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# TODO: calculate a more correct 'episode_id' upon merge\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder \u001b[38;5;241m=\u001b[39m video_recorder\u001b[38;5;241m.\u001b[39mVideoRecorder(\n\u001b[0;32m    257\u001b[0m     env\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv,\n\u001b[0;32m    258\u001b[0m     base_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_video_enabled(),\n\u001b[0;32m    266\u001b[0m )\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_recorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:151\u001b[0m, in \u001b[0;36mVideoRecorder.capture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_ansi_frame(frame)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_image_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:208\u001b[0m, in \u001b[0;36mVideoRecorder._encode_image_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_image_frame\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder:\n\u001b[1;32m--> 208\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mImageEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframes_per_sec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_frames_per_sec\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_version\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mversion_info\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fight\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:336\u001b[0m, in \u001b[0;36mImageEncoder.__init__\u001b[1;34m(self, output_path, frame_shape, frames_per_sec, output_frames_per_sec)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m=\u001b[39m imageio_ffmpeg\u001b[38;5;241m.\u001b[39mget_ffmpeg_exe()\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`. Alternatively, please install imageio-ffmpeg with `pip install imageio-ffmpeg`\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m     )\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`. Alternatively, please install imageio-ffmpeg with `pip install imageio-ffmpeg`"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import retro\n",
    "from gym import Env, spaces\n",
    "import numpy as np\n",
    "import cv2\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Custom environment definition\n",
    "class StreetFighter(Env):\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self.action_space = spaces.MultiBinary(12)\n",
    "        self.render_mode = render_mode\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "        self.metadata = {'render.modes': ['human', 'rgb_array']}  # Set metadata for rendering\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "        self.score = 0\n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84, 84, 1))\n",
    "        return channels\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs)\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'rgb_array':\n",
    "            return self.game.render(mode)\n",
    "        elif mode == 'human':\n",
    "            self.game.render(mode)\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "# Mocking a spec for our custom environment to avoid warnings\n",
    "class EnvSpec:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "\n",
    "StreetFighter.spec = EnvSpec(\"StreetFighter-v0\")\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = StreetFighter(render_mode='human')\n",
    "env = Monitor(env, './logs/', force=True)  # Wrap the environment with Monitor and force clear previous files\n",
    "\n",
    "# Define the model\n",
    "model = PPO('CnnPolicy', env, verbose=1, learning_rate=0.0003, n_steps=2048, batch_size=64, n_epochs=10, gamma=0.99)\n",
    "\n",
    "# Define a callback for evaluation\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/', log_path='./logs/', eval_freq=5000, n_eval_episodes=5, render=False)\n",
    "\n",
    "# Train the model\n",
    "timesteps = 1000000  # Adjust the number of timesteps based on your needs\n",
    "model.learn(total_timesteps=timesteps, callback=eval_callback)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"PPO_StreetFighter\")\n",
    "\n",
    "# Close the training environment to avoid emulator conflict\n",
    "env.close()\n",
    "\n",
    "# Create a new evaluation environment and wrap it with Monitor\n",
    "eval_env = StreetFighter(render_mode='human')\n",
    "eval_env = Monitor(eval_env, './logs/', force=True)\n",
    "\n",
    "# Load the trained model\n",
    "model = PPO.load(\"PPO_StreetFighter\")\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, _ = evaluate_policy(model, eval_env, render=True, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward}\")\n",
    "\n",
    "# Close the evaluation environment\n",
    "eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea6fb5-7269-4991-a793-8a8140778107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
